# ã€Šè¶…å¤§è§„æ¨¡æ“ä½œæ‰‹å†Œï¼šåœ¨ GPU é›†ç¾¤ä¸Šè®­ç»ƒ ã€‹Part5(å¹¶è¡Œç¼–ç¨‹é€Ÿæˆ)**

ä½œè€…ï¼šnanotron

æ ¡æ­£ï¼špprp

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image.png)

## å¹¶è¡Œç¼–ç¨‹é€Ÿæˆ

å°†LLMè®­ç»ƒä»å•ä¸ªGPUæ‰©å±•åˆ°æ•°ç™¾ä¸ªGPUéœ€è¦åœ¨æ‰€æœ‰æœºå™¨ä¹‹é—´è¿›è¡Œæƒé‡ã€æ¢¯åº¦å’Œæ•°æ®çš„é€šä¿¡ä¸åŒæ­¥ã€‚æœ‰ä¸€ç»„åˆ†å¸ƒå¼æ¨¡å¼å¯ä»¥å®ç°è¿™ä¸€ç‚¹ï¼Œç§°ä¸º***é›†ä½“æ“ä½œ Collective Operation***ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œå°†è¿›è¡Œä¸€ä¸ªå°å‹çš„é€Ÿæˆè¯¾ç¨‹ï¼Œæ¶µç›–è¯¸å¦‚*å¹¿æ’­ BroadCastã€å…¨å±€å½’çº¦ AllReduceã€åˆ†æ•£ Scatter* ç­‰æ“ä½œã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰è®¸å¤šç‹¬ç«‹çš„èŠ‚ç‚¹ï¼Œå¯ä»¥æ˜¯CPUæ ¸å¿ƒã€GPUæˆ–è®¡ç®—èŠ‚ç‚¹ã€‚æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œä¸€äº›è®¡ç®—ï¼Œç„¶åæˆ‘ä»¬å¸Œæœ›å°†ç»“æœæˆ–å…¶éƒ¨åˆ†ä¼ è¾“åˆ°å…¶ä»–èŠ‚ç‚¹ï¼Œç”¨äºä¸‹ä¸€ä¸ªè®¡ç®—æ­¥éª¤ï¼ˆt+1ï¼‰ã€‚

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%201.png)

ä¹Ÿè®¸æˆ‘ä»¬éœ€è¦å°†ä¸€ä¸ªèŠ‚ç‚¹çš„ç»“æœå‘é€åˆ°æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ï¼Œæˆ–è€…éœ€è¦æ±‡æ€»æ¯ä¸ªèŠ‚ç‚¹çš„æ‰€æœ‰ä¸­é—´ç»“æœä»¥æŠ¥å‘Šæ€»ä½“ç»“æœã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæœ‰ä¸€ä¸ªå…·æœ‰**æ˜¾è‘—åœ°ä½çš„èŠ‚ç‚¹**åœ¨æ“ä½œä¸­èµ·åˆ°æ ¸å¿ƒä½œç”¨ï¼Œåœ¨è¿™é‡Œç”¨`root`è¡¨ç¤ºï¼Œå®ƒæ˜¯æŸäº›æ“ä½œçš„ç›®æ ‡æˆ–æºã€‚è®©æˆ‘ä»¬ä»æœ€ç®€å•çš„åŸè¯­ä¹‹ä¸€å¼€å§‹ï¼šå¹¿æ’­æ“ä½œ Broadcastã€‚

**å¹¿æ’­ï¼ˆBroadcastï¼‰**Â 

ä¸€ä¸ªéå¸¸å¸¸è§çš„æ¨¡å¼æ˜¯ï¼Œæ‚¨åœ¨èŠ‚ç‚¹1ä¸Šæœ‰ä¸€äº›æ•°æ®ï¼Œå¹¶å¸Œæœ›ä¸æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹å…±äº«æ•°æ®ï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥ä½¿ç”¨æ•°æ®è¿›è¡Œä¸€äº›è®¡ç®—ã€‚å¹¿æ’­æ“ä½œæ­£æ˜¯åšåˆ°äº†è¿™ä¸€ç‚¹ï¼š

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%202.png)

PyTorchåŸç”Ÿæä¾›äº†é›†ä½“æ“ä½œ Collective Operationï¼Œå› æ­¤å¯ä»¥å¾ˆå®¹æ˜“åœ°ç¼–å†™ä¸€ä¸ªå°ä¾‹å­æ¥æ¼”ç¤ºå¹¿æ’­æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚æˆ‘ä»¬é¦–å…ˆéœ€è¦ä½¿ç”¨`dist.init_process_group`åˆå§‹åŒ–ä¸€ä¸ªè¿›ç¨‹ç»„ï¼Œè®¾ç½®é€šä¿¡åç«¯ï¼ˆç¨åæˆ‘ä»¬å°†è®¨è®ºNCCLï¼‰ï¼Œç¡®å®šå­˜åœ¨å¤šå°‘ä¸ª Workersï¼ˆaka Nodesï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªå·¥ä½œè€…åˆ†é…ä¸€ä¸ªRankï¼ˆæˆ‘ä»¬å¯ä»¥ç”¨`dist.get_rank`è·å–ï¼‰ã€‚æœ€åï¼Œå®ƒåœ¨å·¥ä½œè€…ä¹‹é—´å»ºç«‹è¿æ¥ã€‚

ä¸ºäº†å±•ç¤º`dist.broadcast`æ“ä½œï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¼ é‡ï¼Œåœ¨`rank=0`ä¸Šæœ‰éé›¶å€¼ï¼Œå¹¶åœ¨å…¶ä»–å·¥ä½œè€…ä¸Šåˆ›å»ºå…¨é›¶å¼ é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨`dist.broadcast(tensor, src=0)`å°†`rank=0`çš„å¼ é‡åˆ†å‘åˆ°æ‰€æœ‰å…¶ä»–æ’åï¼š

```python
import torch
import torch.distributed as dist

def init_process():
    dist.init_process_group(backend='nccl')
    torch.cuda.set_device(dist.get_rank())
    
def example_broadcast():
    if dist.get_rank() == 0: # root
        tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32).cuda()
    else:
        tensor = torch.zeros(5, dtype=torch.float32).cuda()
    print(f"Before broadcast on rank {dist.get_rank()}: {tensor}")
    dist.broadcast(tensor, src=0)
    print(f"After broadcast on rank {dist.get_rank()}: {tensor}")    
init_process()
example_broadcast()
```

æ‚¨å¯ä»¥ä½¿ç”¨`torchrun --nproc_per_node=3 dist_op.py`è¿è¡Œä¸Šè¿°è„šæœ¬ï¼ˆæ‚¨éœ€è¦3ä¸ªGPUï¼Œæˆ–è€…æ ¹æ®éœ€è¦æ›´æ”¹`nproc_per_node`ï¼‰ï¼Œæ‚¨åº”è¯¥çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºï¼š

```python
Before broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device='cuda:0')
Before broadcast on rank 1: tensor([0., 0., 0., 0., 0.], device='cuda:1')
Before broadcast on rank 2: tensor([0., 0., 0., 0., 0.], device='cuda:2')

After broadcast on rank 0: tensor([1., 2., 3., 4., 5.], device='cuda:0')
After broadcast on rank 1: tensor([1., 2., 3., 4., 5.], device='cuda:1')
After broadcast on rank 2: tensor([1., 2., 3., 4., 5.], device='cuda:2')
```

å¾ˆå¥½ï¼Œçœ‹èµ·æ¥æ­£å¦‚é¢„æœŸçš„é‚£æ ·å·¥ä½œã€‚è¯·æ³¨æ„ï¼ŒRank messageå¯èƒ½ä¼šä»¥æ— åºçš„æ–¹å¼æ‰“å°å‡ºæ¥ï¼Œå› ä¸ºæ— æ³•æ§åˆ¶å“ªä¸ªæ‰“å°è¯­å¥é¦–å…ˆæ‰§è¡Œã€‚ç°åœ¨è®©æˆ‘ä»¬ç»§ç»­è¿›è¡Œå½’çº¦å’Œå…¨å±€å½’çº¦æ¨¡å¼ï¼

### **å½’çº¦ & å…¨å±€å½’çº¦ï¼ˆReduce & AllReduceï¼‰**

å½’çº¦æ¨¡å¼ `Reduce` æ˜¯åˆ†å¸ƒå¼æ•°æ®å¤„ç†ä¸­æœ€åŸºæœ¬çš„æ¨¡å¼ä¹‹ä¸€ã€‚å…¶æ€æƒ³æ˜¯é€šè¿‡ä¸€ä¸ªå‡½æ•°`f()`ï¼ˆä¾‹å¦‚æ±‚å’Œæˆ–å¹³å‡ï¼‰æ¥ç»„åˆæ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„æ•°æ®ã€‚åœ¨å½’çº¦ Reduce çš„ä¾‹å­ä¸­ï¼Œç»“æœä»…å‘é€åˆ° `root`ï¼Œè€Œåœ¨å…¨å±€å½’çº¦ `AllReduce` æƒ…å†µä¸‹ï¼Œç»“æœå¹¿æ’­åˆ°æ‰€æœ‰èŠ‚ç‚¹ï¼š

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%203.png)

å½“ç„¶ï¼Œå¹¶ä¸å­˜åœ¨ä¸€ç§ç¥å¥‡çš„â€œè‡ªç”±è¿è¡Œâ€èŠ‚ç‚¹ï¼Œèƒ½å¤Ÿç‹¬è‡ªå®Œæˆè¿™æ ·çš„è¿ç®—ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨èŠ‚ç‚¹æ‰€æ„æˆçš„ç¯å½¢ Ring æˆ–æ ‘å½¢ Tree ç»“æ„ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½ä¼šè¿›è¡Œä¸€éƒ¨åˆ†è®¡ç®—ã€‚ä¸‹é¢ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼šå‡è®¾æˆ‘ä»¬è¦åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè®¡ç®—ä¸€ç»„æ•°å­—çš„æ€»å’Œï¼Œå¹¶ä¸”è¿™äº›èŠ‚ç‚¹ä»¥ç¯å½¢æ–¹å¼è¿æ¥ã€‚ç¬¬ä¸€ä¸ªèŠ‚ç‚¹å°†è‡ªèº«çš„æ•°å­—å‘é€ç»™ç›¸é‚»èŠ‚ç‚¹ï¼Œè¯¥ç›¸é‚»èŠ‚ç‚¹ä¼šæŠŠæ¥æ”¶åˆ°çš„æ•°å­—ä¸è‡ªå·±çš„æ•°å­—ç›¸åŠ ï¼Œç„¶åå†è½¬å‘ç»™ä¸‹ä¸€ä¸ªç›¸é‚»èŠ‚ç‚¹ã€‚å½“æ²¿ç€èŠ‚ç‚¹ç¯å®Œæˆä¸€è½®ä¼ é€’åï¼Œç¬¬ä¸€ä¸ªèŠ‚ç‚¹å°†ä¼šæ”¶åˆ°æ€»å’Œã€‚ 

è¿™æ˜¯è¿è¡Œç®€å•çš„Reduceæ“ä½œæ¥è®¡ç®—å¼ é‡æ€»å’Œçš„ä»£ç ï¼Œæˆ‘ä»¬ä½¿ç”¨`op=dist.ReduceOp.SUM`æŒ‡å®šè¦ä½¿ç”¨çš„æ“ä½œï¼ˆæ‚¨å¯ä»¥åœ¨[Pytorchæ–‡æ¡£](https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp) [1] ä¸­æ‰¾åˆ°æœ‰å…³æ”¯æŒæ“ä½œçš„æ›´å¤šä¿¡æ¯ï¼‰ï¼š

```python
def example_reduce():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    print(f"Before reduce on rank {dist.get_rank()}: {tensor}")
    # dst=0 ä»£è¡¨rootèŠ‚ç‚¹
    dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)
    print(f"After reduce on rank {rank}: {tensor}")
    
init_process()
example_reduce()
```

è¯·æ³¨æ„ï¼Œåœ¨Reduceæ“ä½œä¸­ï¼Œä»…æ›´æ–°äº†`dst`èŠ‚ç‚¹ä¸Šçš„å¼ é‡ï¼š

```python
Before reduce on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After reduce on rank 0: tensor([6., 6., 6., 6., 6.], device='cuda:0')
After reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
After reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')
```

ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡ŒAllReduceæ“ä½œï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦æŒ‡å®šç›®æ ‡åœ°ç‚¹ï¼‰ï¼š

```python
def example_all_reduce():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    print(f"Before all_reduce on rank {dist.get_rank()}: {tensor}")
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    print(f"After all_reduce on rank {dist.get_rank()}: {tensor}")
    
init_process()
example_all_reduce()
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç»“æœåœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šéƒ½å¯ç”¨ï¼š

```python
Before all_reduce on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before all_reduce on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before all_reduce on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After all_reduce on rank 0: tensor([6., 6., 6., 6., 6.], device='cuda:0')
After all_reduce on rank 1: tensor([6., 6., 6., 6., 6.], device='cuda:1')
After all_reduce on rank 2: tensor([6., 6., 6., 6., 6.], device='cuda:2')
```

ç°åœ¨è®©æˆ‘ä»¬è½¬å‘ä¸‹ä¸€ä¸ªåˆ†å¸ƒå¼é€šä¿¡æ“ä½œã€‚åœ¨è®¸å¤šå®é™…æƒ…å†µä¸‹ï¼Œæ¯ä¸ªèŠ‚ç‚¹å•ç‹¬æ‰§è¡Œè®¸å¤šå¤æ‚çš„è®¡ç®—ï¼Œæˆ‘ä»¬éœ€è¦åœ¨èŠ‚ç‚¹ä¹‹é—´å…±äº«æœ€ç»ˆç»“æœã€‚Gatherå’ŒAllGatheræ˜¯æˆ‘ä»¬åœ¨è¿™ç§æƒ…å†µä¸‹è¦ä½¿ç”¨çš„æ“ä½œã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ï¼

### **Gather & AllGatherÂ èšé›† & å…¨èšé›†**

Gatherå’ŒAllGatherä¸Broadcastéå¸¸ç›¸ä¼¼ï¼Œå› ä¸ºå®ƒä»¬å…è®¸åœ¨èŠ‚ç‚¹ä¹‹é—´åˆ†å‘æ•°æ®è€Œä¸ä¿®æ”¹ã€‚ä¸Broadcastçš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œæˆ‘ä»¬**ä¸éœ€è¦ä»ä¸€ä¸ªèŠ‚ç‚¹å‘æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹å…±äº«ä¸€ä¸ªå€¼(aka Broadcast)**ï¼Œè€Œæ˜¯**æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ªæˆ‘ä»¬å¸Œæœ›æ”¶é›†æ‰€æœ‰æ•°æ®çš„ä¸ªä½“æ•°æ®å—ï¼ˆaka Gatherï¼‰**æˆ–åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šæ”¶é›†æ‰€æœ‰æ•°æ®çš„ä¸ªä½“æ•°æ®å—ï¼ˆåœ¨AllGatherçš„æƒ…å†µä¸‹ï¼‰ã€‚ä¸€å›¾èƒœåƒè¨€ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ï¼š

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%204.png)

è¯·æ³¨æ„ï¼Œè™šçº¿è¡¨ç¤ºæŸäº›æ•°æ®å®é™…ä¸Šæ ¹æœ¬ä¸ç§»åŠ¨ï¼ˆå› ä¸ºå®ƒå·²ç»å­˜åœ¨äºèŠ‚ç‚¹ä¸Šï¼‰ã€‚

åœ¨gatheræ“ä½œçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€ä¸ªå®¹å™¨å¯¹è±¡ï¼Œç”¨äºå­˜å‚¨èšåˆå¼ é‡ï¼Œä¾‹å¦‚`gather_list`ï¼š

```python
def example_gather():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    if dist.get_rank() == 0:
        gather_list = [
            torch.zeros(5, dtype=torch.float32).cuda()
            for _ in range(dist.get_world_size())
            ]
    else:
        gather_list = None
    print(f"Before gather on rank {dist.get_rank()}: {tensor}")
    dist.gather(tensor, gather_list, dst=0)
    if dist.get_rank() == 0:
        print(f"After gather on rank 0: {gather_list}")
    
init_process()
example_gather()
```

æˆ‘ä»¬çœ‹åˆ°`gather_list`ç¡®å®åŒ…å«æ‰€æœ‰æ’åçš„å¼ é‡ï¼š

```python
Before gather on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before gather on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before gather on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After gather on rank 0: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                         tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                         tensor([3., 3., 3., 3., 3.], device='cuda:0')]
```

> ç¼–è€…æ³¨ï¼š dist.gather() çš„å·¥ä½œæ–¹å¼ç¡®å®ä¸éœ€è¦æ˜¾å¼æŒ‡å®š source çš„é¡ºåºï¼Œå› ä¸ºå®ƒä¼šè‡ªåŠ¨æŒ‰ç…§è¿›ç¨‹çš„ rank é¡ºåºæ”¶é›†æ•°æ®ã€‚
> 

å¯¹äºAllGatherç¤ºä¾‹ï¼Œå”¯ä¸€éœ€è¦æ”¹å˜çš„æ˜¯æ¯ä¸ªèŠ‚ç‚¹éƒ½éœ€è¦ä¸€ä¸ªç»“æœçš„å ä½ç¬¦ï¼š

```python
def example_all_gather():
    tensor = torch.tensor([dist.get_rank() + 1] * 5, dtype=torch.float32).cuda()
    gather_list = [
        torch.zeros(5, dtype=torch.float32).cuda()
        for _ in range(dist.get_world_size())
        ]
    print(f"Before all_gather on rank {dist.get_rank()}: {tensor}")
    dist.all_gather(gather_list, tensor)
    print(f"After all_gather on rank {dist.get_rank()}: {gather_list}")
    
init_process()
example_all_gather()
```

ç¡®å®ï¼Œå¯ä»¥çœ‹åˆ°ç°åœ¨æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰äº†æ‰€æœ‰æ•°æ®ï¼š

```python
Before all_gather on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
Before all_gather on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
Before all_gather on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')

After all_gather on rank 0: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                             tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                             tensor([3., 3., 3., 3., 3.], device='cuda:0')]
After all_gather on rank 1: [tensor([1., 1., 1., 1., 1.], device='cuda:1'),
                             tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                             tensor([3., 3., 3., 3., 3.], device='cuda:0')]
After all_gather on rank 2: [tensor([1., 1., 1., 1., 1.], device='cuda:2'),
                             tensor([2., 2., 2., 2., 2.], device='cuda:2'),
                             tensor([3., 3., 3., 3., 3.], device='cuda:2')]
```

é‚£ä¹ˆåå‘æ“ä½œgatheråˆæ˜¯ä»€ä¹ˆå‘¢ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰æ•°æ®éƒ½é›†ä¸­åœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œå¹¶å¸Œæœ›åœ¨èŠ‚ç‚¹ä¹‹é—´åˆ†å‘/åˆ‡ç‰‡å®ƒï¼Œå¯èƒ½è¿˜ä¼šè¿›è¡Œä¸€äº›ä¸­é—´å¤„ç†ï¼Ÿæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Scatterï¼Œæˆ–è€…åœ¨æ“ä½œä¹‹é—´ä½¿ç”¨ReduceScatteræ¨¡å¼ï¼š

### **Scatter & ReduceScatter**

æ­£å¦‚åç§°æ‰€æš—ç¤ºçš„ï¼ŒScatteræ“ä½œçš„ç›®æ ‡æ˜¯å°†æ•°æ®ä»ä¸€ä¸ªèŠ‚ç‚¹åˆ†å‘åˆ°æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ã€‚å› æ­¤ï¼Œå®ƒä¸Broadcastæ“ä½œä¸åŒï¼Œåè€…æ˜¯å¤åˆ¶æ•°æ®è€Œä¸è¿›è¡Œåˆ‡ç‰‡ Slicingï¼Œå¹¶ä¸”å®ƒé€»è¾‘ä¸Šæ˜¯Gatheræ“ä½œçš„åå‘ã€‚

ReduceScatteræ¨¡å¼ç•¥å¾®å¤æ‚ï¼šæƒ³è±¡ä¸€ä¸‹ï¼Œåœ¨Reduceæƒ…å†µä¸‹åº”ç”¨æ“ä½œï¼Œä½†æˆ‘ä»¬ä¸ä»…å°†ç»“æœç§»åŠ¨åˆ°ä¸€ä¸ªèŠ‚ç‚¹ï¼Œè¿˜å°†å…¶å‡åŒ€åˆ†å¸ƒåˆ°æ‰€æœ‰èŠ‚ç‚¹ï¼š

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%205.png)

Scatteræ“ä½œåœ¨ä»£ç ä¸­çš„è¡¨ç¤ºæ–¹å¼ä¸Gatherç›¸åï¼šæˆ‘ä»¬å‡†å¤‡æºæ•°æ®ä½œä¸ºæˆ‘ä»¬å¸Œæœ›åˆ†å‘çš„å¼ é‡åˆ—è¡¨ï¼Œè€Œä¸æ˜¯å‡†å¤‡ä¸€ä¸ªå¼ é‡åˆ—è¡¨ä½œä¸ºç›®æ ‡ã€‚è¿˜éœ€è¦æŒ‡å®š`src`ï¼š

```python
def example_scatter():
    if dist.get_rank() == 0:
        scatter_list = [
            torch.tensor([i + 1] * 5, dtype=torch.float32).cuda()
            for i in range(dist.get_world_size())
            ]
        print(f"Rank 0: Tensor to scatter: {scatter_list}")
    else:
        scatter_list = None
    tensor = torch.zeros(5, dtype=torch.float32).cuda()
    print(f"Before scatter on rank {dist.get_rank()}: {tensor}")
    dist.scatter(tensor, scatter_list, src=0)
    print(f"After scatter on rank {dist.get_rank()}: {tensor}")
    
init_process()
example_scatter()
```

ç»“æœæ˜¾ç¤ºï¼Œç©ºå¼ é‡å·²è¢«`scatter_list`çš„å†…å®¹å¡«å……ï¼š

```python
Rank 0: Tensor to scatter: [tensor([1., 1., 1., 1., 1.], device='cuda:0'),
                            tensor([2., 2., 2., 2., 2.], device='cuda:0'),
                            tensor([3., 3., 3., 3., 3.], device='cuda:0')]
Before scatter on rank 0: tensor([0., 0., 0., 0., 0.], device='cuda:0')
Before scatter on rank 1: tensor([0., 0., 0., 0., 0.], device='cuda:1')
Before scatter on rank 2: tensor([0., 0., 0., 0., 0.], device='cuda:2')

After scatter on rank 0: tensor([1., 1., 1., 1., 1.], device='cuda:0')
After scatter on rank 1: tensor([2., 2., 2., 2., 2.], device='cuda:1')
After scatter on rank 2: tensor([3., 3., 3., 3., 3.], device='cuda:2')
```

è®©æˆ‘ä»¬åˆ›å»ºæ›´æœ‰è¶£çš„æ•°æ®æ¥æ¼”ç¤ºReduceScatterçš„é€»è¾‘ï¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«å¹‚æŒ‡æ•°å’ŒèŠ‚ç‚¹æ’ååç§»å‡½æ•°çš„2å…ƒç´ å‘é‡åˆ—è¡¨ï¼ˆè¿™æœ‰ç‚¹éš¾ä»¥æƒ³è±¡ï¼Œæ‰€ä»¥çœ‹ä¸‹é¢çš„ç¤ºä¾‹ï¼‰ï¼š

```python
def example_reduce_scatter():
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    input_tensor = [
        torch.tensor([(rank + 1) * i for i in range(1, 3)], dtype=torch.float32).cuda()**(j+1) 
        for j in range(world_size)
        ]
    output_tensor = torch.zeros(2, dtype=torch.float32).cuda()
    print(f"Before ReduceScatter on rank {rank}: {input_tensor}")
    dist.reduce_scatter(output_tensor, input_tensor, op=dist.ReduceOp.SUM)
    print(f"After ReduceScatter on rank {rank}: {output_tensor}")    
    
init_process()
example_reduce_scatter()
```

è®©æˆ‘ä»¬æ‰“å°ä¸€ä¸‹æˆ‘ä»¬åˆ›å»ºçš„æ•°æ®æ¨¡å¼ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ç«‹å³çœ‹åˆ°ReduceScatterçš„æ¨¡å¼ï¼šç¬¬ä¸€ä¸ªæ’åæ¥æ”¶äº†æ¯ä¸ªèŠ‚ç‚¹çš„ç¬¬ä¸€ä¸ªå¼ é‡çš„æ€»å’Œï¼Œç¬¬äºŒä¸ªæ’ååŒ…å«äº†æ¯ä¸ªèŠ‚ç‚¹çš„ç¬¬äºŒä¸ªå¼ é‡çš„æ€»å’Œï¼Œä¾æ­¤ç±»æ¨ï¼š

```python
Before ReduceScatter on rank 0: [tensor([1., 2.], device='cuda:0'),
											 tensor([1., 4.], device='cuda:0'),
											 tensor([1., 8.], device='cuda:0')]
Before ReduceScatter on rank 1: [tensor([2., 4.], device='cuda:1'),
                                 tensor([ 4., 16.], device='cuda:1'),
                                 tensor([ 8., 64.], device='cuda:1')]
Before ReduceScatter on rank 2: [tensor([3., 6.], device='cuda:2'),
                                 tensor([ 9., 36.], device='cuda:2'),
                                 tensor([ 27., 216.], device='cuda:2')]

After ReduceScatter on rank 0: tensor([ 6., 12.], device='cuda:0')
After ReduceScatter on rank 1: tensor([14., 56.], device='cuda:1')
After ReduceScatter on rank 2: tensor([ 36., 288.], device='cuda:2')
```

ä¸‹é¢ç®€è¦åœ°çœ‹ä¸€ä¸‹ä¸€ä¸ªå¸¸è§çš„ä½¿ç”¨ReduceScatterå’ŒAllGatherçš„AllReduceå®ç°ï¼šRing AllReduceã€‚

### **å¿«é€Ÿå…³æ³¨Ring AllReduce**

***ç¯å½¢ Ring AllReduce***æ˜¯AllReduceçš„ä¸€ç§ç‰¹å®šå®ç°ï¼Œç»è¿‡ä¼˜åŒ–ä»¥å®ç°å¯ä¼¸ç¼©æ€§ã€‚ä¸æ‰€æœ‰è®¾å¤‡ç›´æ¥ç›¸äº’é€šä¿¡ä¸åŒ(è¿™å¯èƒ½ä¼šé€ æˆé€šä¿¡ç“¶é¢ˆ)ï¼Œç¯å½¢All-Reduceå¯ä»¥åˆ†è§£ä¸ºä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šReduceScatterå’ŒAllGatherã€‚å®ƒçš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š

1. **ReduceScatter**Â 
    - æ¯ä¸ªè®¾å¤‡å°†å…¶æ•°æ®ï¼ˆä¾‹å¦‚æ¢¯åº¦ï¼‰åˆ†å‰²æˆå—ï¼Œå¹¶å°†ä¸€ä¸ªå—å‘é€ç»™å…¶é‚»å±…ã€‚åŒæ—¶ï¼Œæ¯ä¸ªè®¾å¤‡ä»å…¶å¦ä¸€ä¸ªé‚»å±…æ¥æ”¶ä¸€ä¸ªå—ã€‚
    - å½“æ¯ä¸ªè®¾å¤‡æ¥æ”¶åˆ°ä¸€ä¸ªå—æ—¶ï¼Œå®ƒå°†å…¶å¯¹åº”çš„å—æ·»åŠ ï¼ˆå‡å°‘ï¼‰åˆ°æ¥æ”¶åˆ°çš„å—ä¸­ã€‚
    - è¿™ä¸ªè¿‡ç¨‹åœ¨ç¯ä¸­æŒç»­è¿›è¡Œï¼Œç›´åˆ°æ¯ä¸ªè®¾å¤‡æŒæœ‰ä¸€ä¸ªéƒ¨åˆ†å‡å°‘çš„å—ï¼Œè¡¨ç¤ºè¯¥å—çš„æ¢¯åº¦åœ¨æ‰€æœ‰è®¾å¤‡ä¸­çš„æ€»å’Œã€‚
2. **AllGather**Â 
    - ç°åœ¨ï¼Œæ¯ä¸ªè®¾å¤‡éœ€è¦ä»å…¶ä»–è®¾å¤‡æ”¶é›†å®Œå…¨å‡å°‘çš„å—ã€‚
    - è®¾å¤‡å¼€å§‹å°†å®ƒä»¬çš„å‡å°‘å—å‘é€ç»™é‚»å±…ã€‚
    - æ¯ä¸ªè®¾å¤‡è½¬å‘æ”¶åˆ°çš„å—ï¼Œç›´åˆ°æ¯ä¸ªè®¾å¤‡éƒ½æœ‰äº†å®Œå…¨å‡å°‘çš„å—ï¼Œä½¿æ¯ä¸ªè®¾å¤‡å¾—åˆ°å®Œæ•´çš„ã€æ€»ç»“çš„æ¢¯åº¦ã€‚

è®©æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹åŠ¨ç”»æ¥è¯´æ˜ï¼Œæˆ‘ä»¬æœ‰5ä¸ªGPUï¼Œæ¯ä¸ªGPUéƒ½æœ‰é•¿åº¦ä¸º5çš„å¼ é‡ã€‚ç¬¬ä¸€ä¸ªåŠ¨ç”»æ˜¾ç¤ºäº†ReduceScatteræ­¥éª¤ï¼Œæœ€ç»ˆæ¯ä¸ªGPUéƒ½æ¥æ”¶åˆ°äº†ç‰¹å®šæ•°æ®å—çš„å‡å°‘ç»“æœï¼ˆæ©™è‰²çŸ©å½¢ï¼‰ï¼š

> ç¼–è€…æ³¨ï¼šä»¥ä¸‹ä¸¤å¼ å‡ä¸ºgifå›¾ï¼Œå»ºè®®è®¿é—®ç½‘ç«™æ¥å¾—åˆ°æ›´å¥½çš„é˜…è¯»ä½“éªŒ
> 

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%206.png)

æ¥ä¸‹æ¥çš„åŠ¨ç”»å±•ç¤ºäº†AllGatheræ­¥éª¤ï¼Œåœ¨æ­¤è¿‡ç¨‹ç»“æŸæ—¶ï¼Œæ¯ä¸ªGPUè·å–äº†AllReduceæ“ä½œçš„å®Œæ•´ç»“æœ(å³ä¸Šæ–‡æåˆ°çš„ï¼šAllReduce=ReduceScatter + AllGather)ï¼š

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%207.png)

æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œåœ¨reduce-scatterå’Œall-gatheræ­¥éª¤ä¸­ï¼Œæ¯ä¸ªGPUå‘é€å’Œæ¥æ”¶å€¼ $Nâˆ’1$æ¬¡ã€‚æ¯ä¸ªGPUæ¯æ¬¡ä¼ è¾“å‘é€ $K/N$ ä¸ªå€¼ï¼Œå…¶ä¸­ *K* æ˜¯æ•°ç»„é•¿åº¦ã€‚å› æ­¤ï¼Œæ¯ä¸ªGPUå‘é€å’Œæ¥æ”¶çš„æ€»æ•°æ®é‡ä¸º $2Ã—(Nâˆ’1)Ã—K/N$ã€‚å½“ *N*ï¼ˆGPUçš„æ•°é‡ï¼‰è¾ƒå¤§æ—¶ï¼Œæ¯ä¸ªGPUå‘é€å’Œæ¥æ”¶çš„æ€»æ•°æ®é‡çº¦ä¸º2Ã—Kï¼Œå…¶ä¸­ *K*æ˜¯æ€»å‚æ•°æ•°é‡ã€‚

**å¯¹äºAllReduceï¼Œæœ‰ä¸¤ä¸ªå…³é”®ç‚¹éœ€è¦è®°ä½ï¼š**

1. å½“*N*ï¼ˆGPUçš„æ•°é‡ï¼‰è¾ƒå¤§æ—¶ï¼ŒAllReduceçš„é€šä¿¡æˆæœ¬çº¦ä¸º2Ã—*K*ã€‚
2. ä¸€ä¸ªAllReduceæ“ä½œå¯ä»¥åˆ†è§£ä¸ºreduce-scatterå’Œall-gatherä¸¤ä¸ªæ­¥éª¤ã€‚è¿™ä¸¤ä¸ªæ“ä½œçš„é€šä¿¡æˆæœ¬æ˜¯AllReduceçš„ä¸€åŠï¼Œçº¦ä¸º*K*ã€‚

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œå³ä½¿åœ¨èŠ‚ç‚¹ä¹‹é—´å¸¦å®½æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¿™ç§å®ç°ä¹Ÿå¯ä»¥æœ‰æ•ˆåˆ©ç”¨ã€‚

ç°åœ¨å·²ç»äº†è§£äº†åˆ†å¸ƒå¼æ“ä½œçš„ä¸»è¦æ„å»ºæ¨¡å—ï¼Œä½†åœ¨å®é™…æ“ä½œä¸­è®©æˆ‘ä»¬çœ‹çœ‹ç”¨äºåŒæ­¥çš„ç‰¹æ®Šæ“ä½œä¹‹å‰ï¼Œæ¥çœ‹çœ‹ä¸€ä¸ªç‰¹æ®Šæ“ä½œï¼šBarrierã€‚

### **BarrierÂ å±éšœ**

Barrieræ˜¯ä¸€ç§ç®€å•çš„æ“ä½œï¼Œç”¨äºåŒæ­¥æ‰€æœ‰èŠ‚ç‚¹ã€‚ç›´åˆ°æ‰€æœ‰èŠ‚ç‚¹éƒ½åˆ°è¾¾Barrierä¹‹å‰ï¼ŒBarrierä¸ä¼šè¢«è§£é™¤ã€‚ç„¶åæ‰èƒ½ç»§ç»­è¿›è¡Œè¿›ä¸€æ­¥çš„è®¡ç®—ï¼š

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%208.png)

æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè®¾ç½®ä¸åŒçš„ç¡çœ æ—¶é—´æ¥è½»æ¾æ¨¡æ‹Ÿå»¶è¿Ÿçš„èŠ‚ç‚¹ï¼Œç„¶åçœ‹çœ‹å®ƒä»¬é€šè¿‡Barrieræ‰€éœ€çš„æ—¶é—´ï¼š

```python
def example_barrier():
    rank = dist.get_rank()
    t_start = time.time()
    print(f"Rank {rank} sleeps {rank} seconds.")
    time.sleep(rank)  # Simulate different processing times
    dist.barrier()
    print(f"Rank {rank} after barrier time delta: {time.time()-t_start:.4f}")
    
init_process()
example_barrier()
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå°½ç®¡ç¬¬ä¸€ä¸ªæ’åæ²¡æœ‰ç¡çœ ï¼Œä½†å®ƒä¹Ÿéœ€è¦2ç§’æ‰èƒ½é€šè¿‡Barrierï¼š

```python
Rank 0 sleeps 0 seconds.
Rank 1 sleeps 1 seconds.
Rank 2 sleeps 2 seconds.

Rank 0 after barrier time delta: 2.0025
Rank 1 after barrier time delta: 2.0025
Rank 2 after barrier time delta: 2.0024
```

éœ€è¦å°å¿ƒåœ°è¿›è¡Œè¿™ç§æ–¹å¼çš„æ‰€æœ‰èŠ‚ç‚¹åŒæ­¥æ“ä½œï¼Œå› ä¸ºè¿™ä¼šæ‰“è´¥å¹¶è¡Œç‹¬ç«‹æ“ä½œçš„ç›®çš„ï¼Œå¯èƒ½ä¼šå‡æ…¢æ•´ä¸ªå¤„ç†é€Ÿåº¦ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå¦‚æœå¿«é€ŸèŠ‚ç‚¹å·²ç»å¼€å§‹å¤„ç†ä¸‹ä¸€ä¸ªä½œä¸šï¼Œè¿™å¯èƒ½æ˜¯å¯ä»¥æ¥å—çš„ï¼Œå› ä¸ºå¿«é€ŸèŠ‚ç‚¹åœ¨ä¸‹ä¸€ä¸ªè¿­ä»£ä¸­å¯èƒ½ä¼šå˜æ…¢ï¼Œä»è€Œå¹³è¡¡æ•´ä¸ªè¿‡ç¨‹ä¸­çš„å»¶è¿Ÿã€‚

åœ¨è½¬å‘å®é™…åˆ†å¸ƒå¼è®­ç»ƒå®ç°ä¹‹å‰ï¼Œå…ˆæ¥äº†è§£ï¼šNCCLåˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ

### **NCCLï¼šNVIDIA Collective Communications LibraryNCCLï¼šNVIDIA é›†ä½“é€šä¿¡åº“**

å½“åœ¨è®¸å¤šGPUä¸Šè®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œç»å¸¸ä¼šé‡åˆ°NCCLï¼é‚£æ˜¯ä»€ä¹ˆï¼Ÿ

æœ‰å‡ ä¸ªå®ç°é›†ä½“é€šä¿¡ Collective Communication çš„åº“ï¼Œå¹¶å¾—åˆ°PyTorchçš„æ”¯æŒï¼šæœ‰ç»å…¸çš„***MPI***ï¼ˆæ¶ˆæ¯ä¼ é€’æ¥å£ï¼‰ï¼Œæœ‰Metaçš„***Gloo***ï¼Œæœ€åè¿˜æœ‰ **NCCL**ï¼ˆNVIDIAé›†ä½“é€šä¿¡åº“ï¼‰ã€‚å®ƒä»¬åœ¨é›†ä½“é€šä¿¡æ¨¡å¼æ–¹é¢æä¾›ç±»ä¼¼çš„åŠŸèƒ½ï¼Œä½†é’ˆå¯¹ä¸åŒçš„ç¡¬ä»¶è®¾ç½®è¿›è¡Œäº†ä¼˜åŒ–ï¼›NCCLè®¾è®¡ç”¨äºæœ‰æ•ˆåœ°æœåŠ¡GPU-GPUé€šä¿¡ï¼Œè€ŒMPIå’ŒGlooåˆ™è®¾ç½®ä¸ºCPU-CPUæˆ–CPU-GPUé€šä¿¡ã€‚PyTorchæä¾›äº†ä¸€ä¸ª[å¾ˆå¥½çš„æŒ‡å—](https://pytorch.org/docs/stable/distributed.html#which-backend-to-use) [2] æ¥å†³å®šä½¿ç”¨å“ªä¸€ä¸ªï¼š

- GPUè®­ç»ƒï¼šä½¿ç”¨NCCL
- CPUè®­ç»ƒï¼šä½¿ç”¨Gloo

### **åˆ†å¸ƒå¼è®­ç»ƒæ€§èƒ½åˆ†æ**

### **å†…æ ¸**

å‡è®¾å†…æ ¸å·²ç»é›†æˆåˆ°PyTorchä¸­ã€‚ä½œä¸ºä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹åœ¨PyTorchä¸­å®ç°çš„Layer Normalizationå‡½æ•°`torch.nn.functional.layer_norm`ã€‚æœ‰å‡ ç§æ–¹æ³•å¯ä»¥åˆ†ææ­¤å‡½æ•°çš„æ ¸å¿ƒã€‚æœ€ç›´æ¥çš„æ–¹æ³•å¯èƒ½æ˜¯ä½¿ç”¨Pythonçš„`time`æ¨¡å—ã€‚ç„¶è€Œï¼Œç”±äºCUDAæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•æµ‹é‡æ—¶é—´åªä¼šæ•è·Pythonä¸­å¯åŠ¨å†…æ ¸çš„å¼€é”€ï¼Œè€Œä¸æ˜¯å†…æ ¸æœ¬èº«çš„å®é™…æ‰§è¡Œæ—¶é—´ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥åˆ©ç”¨`torch.cuda.Event`æ¥è¿›è¡Œå‡†ç¡®çš„æ—¶é—´æµ‹é‡ï¼Œå¹¶ä½¿ç”¨`torch.cuda.synchronize()`æŒ‡ä»¤ç¡®ä¿ç­‰å¾…å†…æ ¸æ‰§è¡Œå®Œæˆã€‚ä»¥ä¸‹ä»£ç æ®µå±•ç¤ºäº†è¿™ç§æ–¹æ³•ï¼š

```python
def profile_pytorch(func, input):
    # åˆ›å»ºCUDAäº‹ä»¶ä»¥è·Ÿè¸ªæ—¶é—´ã€‚CUDAæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œ
    start = torch.cuda.Event(enable_timing=True)  # äº‹ä»¶æ ‡è®°å¼€å§‹æ—¶é—´
    end = torch.cuda.Event(enable_timing=True)    # äº‹ä»¶æ ‡è®°ç»“æŸæ—¶é—´
    # é¢„çƒ­ä»¥æ¶ˆé™¤ç¬¬ä¸€æ¬¡è¿è¡Œçš„ä»»ä½•å¼€é”€ï¼Œè¿™å¯èƒ½ä¸åæ˜ 
    # å®é™…æ€§èƒ½ã€‚
    for _ in range(10):
        func(input)
    # åœ¨æ‰§è¡Œå‡½æ•°ä¹‹å‰è®°å½•å¼€å§‹æ—¶é—´
    start.record()
    func(input)  # è°ƒç”¨æˆ‘ä»¬æƒ³è¦åˆ†æçš„å‡½æ•°
    # åœ¨å‡½æ•°å®Œæˆåè®°å½•ç»“æŸæ—¶é—´
    end.record()
    # åŒæ­¥CUDAæ“ä½œï¼Œä»¥ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆåå†æµ‹é‡
    torch.cuda.synchronize()
    # è®¡ç®—å¹¶è¿”å›è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰ã€‚
    return start.elapsed_time(end)
```

æ›´æœ‰æ•ˆçš„æ€§èƒ½åˆ†ææ–¹æ³•æ˜¯åˆ©ç”¨ä¹‹å‰ä»‹ç»çš„PyTorch Profilerã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘ä»¥ä¸‹ä»£ç ï¼š

```python
import torch
import torch.nn.functional as F

def pytorch_layer_norm(input):
    return F.layer_norm(input, input.size()[1:])

a = torch.randn(10000, 10000).cuda()

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,  # åˆ†æCPUæ´»åŠ¨
        torch.profiler.ProfilerActivity.CUDA,  # åˆ†æCUDAæ´»åŠ¨
    ],
    # å®šä¹‰åˆ†æå™¨çš„è°ƒåº¦
    schedule=torch.profiler.schedule(
        wait=1,      # åœ¨å¼€å§‹åˆ†æä¹‹å‰ç­‰å¾…1æ¬¡è¿­ä»£
        warmup=3,    # è¿›è¡Œ3æ¬¡è¿­ä»£çš„é¢„çƒ­ï¼Œä»¥ç¨³å®šæ€§èƒ½
        active=2,    # è¿›è¡Œ2æ¬¡æ´»åŠ¨è¿­ä»£çš„åˆ†æ
        repeat=1,    # å°†åˆ†æè°ƒåº¦é‡å¤ä¸€æ¬¡
    ),
    on_trace_ready=torch.profiler.tensorboard_trace_handler('.'),

) as p:
    for iter in range(10):
        pytorch_layer_norm(a)
        p.step()

# æ‰“å°æŒ‰æ€»CUDAæ—¶é—´æ’åºçš„æ±‡æ€»åˆ†æç»“æœè¡¨ï¼Œé™åˆ¶æ˜¾ç¤ºå‰8ä¸ªæ¡ç›®
print(p.key_averages().table(sort_by="cuda_time_total", row_limit=8))
```

è¿™å°†æ‰“å°æŒ‰æ€»CUDAæ—¶é—´æ’åºçš„æ±‡æ€»åˆ†æç»“æœè¡¨ï¼Œè¾“å‡ºå¦‚ä¸‹ï¼š

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%209.png)

æ‚¨è¿˜å¯ä»¥å°è¯•åœ¨Â `chrome://tracing/`Â ä¸Šæ£€æŸ¥è·Ÿè¸ªï¼š

**ğŸ’¡ æç¤º**

å¦‚æœæ‚¨æ˜¯ç¬¬ä¸€æ¬¡ä½¿ç”¨è¯¥å·¥å…·ï¼Œå¯ä»¥ä½¿ç”¨å³ç®­å¤´å’Œå·¦ç®­å¤´é”®å¯¼èˆªè·Ÿè¸ªã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜å¯ä»¥æŒ‰ä½**Alt**é”®ï¼ŒåŒæ—¶ä½¿ç”¨é¼ æ ‡å·¦å³æ»šåŠ¨æ¥æ”¾å¤§å’Œç¼©å°ã€‚

æ”¾å¤§åï¼Œå¯ä»¥è§‚å¯Ÿè°ƒç”¨Â `layer_norm`Â æ—¶æ“ä½œæµç¨‹çš„è·Ÿè¸ªï¼š

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%2010.png)

åºåˆ—ä»CPUï¼ˆä¸Šéƒ¨åˆ†ï¼‰å¼€å§‹ï¼Œä½¿ç”¨Â `aten::layer_norm`ï¼Œç„¶åè½¬åˆ°Â `aten::native_layer_norm`ï¼Œæœ€åè¿‡æ¸¡åˆ°Â `cudaLaunchKernel`ã€‚ä»é‚£é‡Œï¼Œæˆ‘ä»¬è¿›å…¥GPUï¼Œè°ƒç”¨Â `vectorized_layer_norm_kernel`Â å†…æ ¸ã€‚

> **ğŸ“ æ³¨æ„** 
å¯ä»¥é€šè¿‡å°†åˆ†æå™¨ä¸­çš„Â `profile_memory`Â è®¾ç½®ä¸ºÂ `True`Â æ¥å¯ç”¨å†…å­˜åˆ†æã€‚ä½†è¿™å¯èƒ½ä¼šå¯¼è‡´æ›´å¤æ‚çš„è·Ÿè¸ªã€‚
> 

è™½ç„¶PyTorch Profileræä¾›äº†å¿«é€Ÿçš„æ€§èƒ½æ¦‚è¿°ï¼Œä½†**NVIDIA Nsight Compute (ncu)**Â æä¾›äº†æ›´æ·±å…¥çš„GPUæ€§èƒ½æ´å¯Ÿï¼ŒåŒ…æ‹¬æ¯ä¸ªå†…æ ¸çš„è¯¦ç»†æ‰§è¡Œæ—¶é—´å’Œå†…å­˜ä½¿ç”¨æƒ…å†µã€‚è¦è¿è¡Œåˆ†æå™¨éå¸¸ç®€å•ï¼š

```bash
ncu --set full python layer_norm.py
```

è¿™é‡Œçš„Â `layer_norm.py`Â æ˜¯æ‰§è¡Œå±‚å½’ä¸€åŒ–å‡½æ•°çš„ç®€å•æ–‡ä»¶ã€‚æ­¤å‘½ä»¤å°†ç”Ÿæˆæ—¥å¿—è¾“å‡ºï¼Œä½†æ›´æœ‰æ•ˆçš„æ–¹æ³•æ˜¯é€šè¿‡è®¾ç½®è¾“å‡ºæ ‡å¿—æ¥å¯è§†åŒ–ç»“æœï¼š

```bash
ncu --set full -o output python layer_norm.py
```

ç„¶åä½¿ç”¨Nsight Computeæ‰“å¼€æ–‡ä»¶Â `output.ncu-rep`ï¼Œæ‚¨å°†çœ‹åˆ°ç±»ä¼¼äºä»¥ä¸‹çš„è§†å›¾ï¼š

![image.png](https://raw.githubusercontent.com/pprp/blogimagebed/main/part_5_image%2011.png)

å…¶ä¸­æ¸…æ™°åœ°æ˜¾ç¤ºäº†å…³äºè®¡ç®—å’Œå†…å­˜åˆ©ç”¨ç‡çš„è­¦å‘Šï¼Œä»¥åŠå¦‚ä½•ä¼˜åŒ–å†…æ ¸ä»¥å®ç°æœ€å¤§å ç”¨ç‡ã€‚

### **CPPæ‰©å±•**

å¦‚æœè¦åˆ†æçš„å†…æ ¸å°šæœªé›†æˆåˆ°PyTorchä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨PyTorchçš„Â `cpp_extension`Â æ¨¡å—è½»æ¾ç¼–è¯‘å’Œè¿è¡Œè‡ªå®šä¹‰CUDAä»£ç ã€‚è¿™ä¸ªè¿‡ç¨‹éå¸¸ç®€å• â€”â€” åªéœ€åœ¨Â `.cu`Â æ–‡ä»¶ä¸­åˆ›å»ºæ‚¨çš„CUDAå†…æ ¸ï¼Œå¹¶ä½¿ç”¨Â `cpp_extension`Â æ¨¡å—ä¸­çš„Â `load`Â å‡½æ•°å°†å…¶åŠ è½½åˆ°Pythonä¸­ã€‚

ä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•çš„Â `add`Â å†…æ ¸çš„Â `.cu`Â æ–‡ä»¶å¦‚ä¸‹ï¼š

```clike
#include 
#include 
#include 

__global__ void add_kernel(float* x, float* y, float* output, int size) {
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    if (index < size) {
        output[index] = x[index] + y[index];
    }
}

void add_cuda(torch::Tensor x, torch::Tensor y, torch::Tensor output) {
    int threads = 1024;
    int blocks = (x.size(0) + threads - 1) / threads;

    add_kernel<<>>(x.data_ptr(), y.data_ptr(), output.data_ptr(), x.size(0));
}
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("add_cuda", &add_cuda, "Vector addition (CUDA)");
}
```

ä»¥åŠç”¨äºåŠ è½½å†…æ ¸çš„Pythonæ–‡ä»¶ï¼š

```python
import torch
from torch.utils.cpp_extension import load

# åŠ è½½å¹¶ç¼–è¯‘CUDAæ‰©å±•
vector_add = load(
    name="vector_add",
    sources=["add_kernel.cu"],
    verbose=True
)

# å®šä¹‰è¾“å…¥å¼ é‡
size = 10000
x = torch.randn(size, device='cuda')
y = torch.randn(size, device='cuda')
output = torch.empty(size, device='cuda')

# è¿è¡ŒCUDAå†…æ ¸
vector_add.add_cuda(x, y, output)
```

ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæ‚¨å¯ä»¥åƒä¹‹å‰å±•ç¤ºçš„é‚£æ ·ï¼Œä½¿ç”¨PyTorchçš„åˆ†æå™¨æˆ–NVIDIAå·¥å…·æ¥åˆ†æè‡ªå®šä¹‰CUDAå†…æ ¸ã€‚

## è®¡ç®—LLM è®­ç»ƒä¸­çš„è§„æ¨¡

è®©æˆ‘ä»¬äº†è§£LLMè®­ç»ƒä¸­çš„å…¸å‹å°ºåº¦ã€‚å½“æˆ‘ä»¬è°ˆè®ºå†…å­˜æˆ–è®¡ç®—æ—¶ï¼Œé€šå¸¸æ˜¯åœ¨è®¡ç®—â€œå…ƒç´ â€ - å¯ä»¥å°†å…¶è§†ä¸ºå¼ é‡ä¸­çš„æ•°å€¼ã€‚è¦å¾—åˆ°å®é™…çš„å†…å­˜å ç”¨ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ï¼Œæ‚¨éœ€è¦ä¹˜ä»¥æ¯ä¸ªæ•°å­—çš„å¤§å°ï¼ˆä¾‹å¦‚ï¼Œbf16ä¸º2å­—èŠ‚ï¼Œfp32ä¸º4å­—èŠ‚ï¼‰ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›å¿«é€Ÿçš„ç²—ç•¥ä¼°è®¡ï¼š

- è¾“å…¥ä»¤ç‰Œï¼ˆInput tokensï¼‰ï¼šå¯¹äºæ¯ä¸ªæ‰¹æ¬¡ï¼Œæˆ‘ä»¬å¤„ç† $\text{seq} \cdot \text{mbs}$ ä¸ªä»¤ç‰Œï¼Œå…¶ä¸­mbsæ˜¯å¾®æ‰¹æ¬¡å¤§å°ï¼Œseqæ˜¯åºåˆ—é•¿åº¦ã€‚
- æ¿€æ´»ï¼ˆéšè—çŠ¶æ€ï¼‰ï¼ˆActivations (hidden states)ï¼‰ï¼šå¯¹äºå•ä¸ªå±‚ï¼Œéšè—çŠ¶æ€å¼ é‡çš„å¤§å°ä¸º $\text{seq} \cdot \text{mbs} \cdot h$ ä¸ªå…ƒç´ ã€‚
- æ¨¡å‹æƒé‡å’Œæ¢¯åº¦ï¼ˆModel weights and gradientsï¼‰ï¼šæ¨¡å‹ä¸­çš„æ¯ä¸ªæƒé‡çŸ©é˜µï¼ˆå¦‚çº¿æ€§å±‚ä¸­çš„ï¼‰å¤§çº¦æœ‰ $h^2$ ä¸ªå…ƒç´ ã€‚è¿™æ˜¯æ¯ä¸ªæƒé‡çŸ©é˜µçš„å…ƒç´ æ•°é‡ã€‚æ¢¯åº¦ä¸æƒé‡çš„å¤§å°ç›¸åŒã€‚
- ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆOptimizer statesï¼‰ï¼šå¯¹äºæ¯ä¸ªæƒé‡çŸ©é˜µï¼ˆå…ƒç´ æ•°é‡ä¸º $h^2$ï¼‰ï¼Œå¦‚æœä½ ä½¿ç”¨åƒAdamè¿™æ ·çš„ä¼˜åŒ–å™¨è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒï¼Œå®ƒä¼šåœ¨fp32ç²¾åº¦ä¸‹ä¿ç•™åŠ¨é‡å’Œæ–¹å·®çŠ¶æ€ï¼ˆ $2 \cdot h^2$ï¼‰ï¼Œä»¥åŠä¸»æƒé‡åœ¨fp32ï¼ˆ $h^2$ï¼‰ã€‚å› æ­¤ï¼Œæ¯ä¸ªæƒé‡çŸ©é˜µçš„æ€»ä¼˜åŒ–å™¨çŠ¶æ€å°†çº¦ä¸º $6 \cdot h^2$ã€‚
- æ€»æ¨¡å‹å‚æ•°ï¼šå¯¹äºæ¯ä¸ªtransformerå—ï¼š
    - æ³¨æ„åŠ›å‚æ•°ï¼š
        - QKVæŠ•å½±ï¼š $3h^2$ å‚æ•°
        - è¾“å‡ºæŠ•å½±ï¼š $h^2$ å‚æ•°
    - å¸¦æœ‰GLUçš„MLPå‚æ•°ï¼š
        - Gateå’ŒUp Projï¼š $8h^2$ å‚æ•°ï¼ˆ2ä¸ªå¤§å°ä¸º $h \times 4h$ çš„çŸ©é˜µï¼‰
        - Down Projï¼š $4h^2$ å‚æ•°ï¼ˆ1ä¸ªå¤§å°ä¸º $4h \times h$ çš„çŸ©é˜µï¼‰
    - æ¯ä¸ªå—çš„æ€»å‚æ•°ï¼šä½¿ç”¨GLU MLPsæ—¶ä¸º $16h^2$ï¼Œä¸ä½¿ç”¨GLUæ—¶ä¸º $12h^2$
    - å¯¹äºå®Œæ•´æ¨¡å‹ï¼š $16h^2 \cdot \text{num\_layers}$ï¼ˆä½¿ç”¨GLUï¼‰
    - é¢å¤–å‚æ•°ï¼š
        - è¾“å…¥åµŒå…¥ï¼š $\text{vocab\_size} \cdot h$
        - LMå¤´ï¼š $\text{vocab\_size} \cdot h$ï¼ˆå¦‚æœä¸ä¸è¾“å…¥åµŒå…¥ç»‘å®šï¼‰
        - ä½ç½®åµŒå…¥ï¼ˆå¦‚æœä½¿ç”¨ï¼‰ï¼š $\text{max\_seq\_len} \cdot h$
- å‰å‘å’Œåå‘ä¼ é€’è®¡ç®—ï¼ˆFLOPsï¼‰ï¼šå‰å‘ä¼ é€’çš„FLOPsçš„éå¸¸ç²—ç•¥çš„ä¼°è®¡ä¸º  $2 \cdot \text{num\_tokens} \cdot \text{num\_params}$ã€‚åå‘ä¼ é€’è®¡ç®—æ˜¯å‰è€…çš„ä¸¤å€ï¼š $4 \cdot \text{num\_tokens} \cdot \text{num\_params}$ã€‚

## **è®¡ç®—/é€šä¿¡é‡å éœ€è¦çš„æ•°å­¦**

ä½¿ç”¨å‰ä¸€èŠ‚ä¸­çš„å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä¼°è®¡åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­è®¡ç®—å’Œé€šä¿¡ä½•æ—¶å¯ä»¥æœ‰æ•ˆé‡å ã€‚è®©æˆ‘ä»¬ä»¥æ•°æ®å¹¶è¡Œï¼ˆZero-0ï¼‰ä¸ºä¾‹ã€‚

### æ•°æ®å¹¶è¡ŒDPé€šä¿¡åˆ†æ

éœ€è¦é€šä¿¡çš„æ€»æ¢¯åº¦å¤§å°ä¸ºï¼š

- æ¢¯åº¦ = å‚æ•° â‰ˆ  $\text{num\_layers} \cdot 16h^2$

åœ¨åå‘ä¼ é€’è¿‡ç¨‹ä¸­ï¼Œè¿™äº›æ¢¯åº¦ä»¥Bucketsï¼ˆé»˜è®¤25MBï¼‰çš„å½¢å¼è¿›è¡Œé€šä¿¡ã€‚æ¯ä¸ªæ¡¶çš„AllReduceé€šä¿¡æ—¶é—´ä¸ºï¼š

$$
t_{\text{comm}} = t_{\text{comm\_bucket}} = \frac{\text{bucket\_size} \cdot 2(\text{DP} - 1)}{\text{DP} \cdot \text{peak\_bw}}
$$

> æ³¨æ„ï¼šå¯¹äºå¸¦å®½è®¡ç®—ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¥è‡ªÂ [NCCLæ–‡æ¡£](https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md#summary) [3]Â çš„æ€»çº¿å¸¦å®½å…¬å¼ã€‚è¿™äº›å…¬å¼è€ƒè™‘äº†åœ¨è®¡ç®—GPUä¹‹é—´è®¡ç®—æœ‰æ•ˆå¸¦å®½æ—¶çš„å…·ä½“é€šä¿¡æ¨¡å¼ã€‚
> 

> ç¼–è€…æ³¨ï¼š Peak_bw ä»£è¡¨ Peak Bandwidth; DPä»£è¡¨DPåº¦ï¼Œå¯ä»¥ç®€å•ç†è§£ä¸ºæœ‰å¤šå°‘å¡ä½¿ç”¨DPã€‚
> 

åå‘ä¼ é€’çš„è®¡ç®—æ—¶é—´ä¸ºï¼š

$$
t_{\text{compute}} = \frac{4 \cdot \text{num\_tokens} \cdot \text{num\_params}}{\text{peak\_flops}}
$$

ä¸ºäº†æœ‰æ•ˆé‡å ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

$$
\frac{t_{\text{comm}}}{t_{\text{compute}}} = \frac{\text{num\_params}}{2 \cdot \text{num\_tokens}} \cdot \frac{\text{DP} - 1}{\text{DP}} \cdot \frac{\text{peak\_flops}}{\text{peak\_bw}} \leq 1
$$

è¿™ä¸ªæ¯”ç‡æœ‰åŠ©äºç¡®å®š**é€šä¿¡æ˜¯å¦ä¼šæˆä¸ºè®­ç»ƒä¸­çš„ç“¶é¢ˆ**ã€‚å½“æ¯”ç‡å°äº1æ—¶ï¼Œé€šä¿¡å¯ä»¥ä¸è®¡ç®—å®Œå…¨é‡å ã€‚

### Zero-3 (FSDP) é€šä¿¡åˆ†æ

å¯¹äº Zero-3ï¼Œå‚æ•°å’Œæ¢¯åº¦åœ¨ GPU ä¹‹é—´å…±äº«ã€‚è®©æˆ‘ä»¬åˆ†æä¸€ä¸ªå…·æœ‰æ¯ä¸ªå¤§å°ä¸º  $16h^2$ å‚æ•°çš„ transformer å—çš„æ¨¡å‹çš„é€šä¿¡æ¨¡å¼ï¼š

- å¯¹äºå‰å‘ä¼ æ’­ä¸­çš„æ¯ä¸ª transformer å—ï¼š
    - AllGatherï¼šæ¯ä¸ª rank $16h^2/DP$ å­—èŠ‚
- å¯¹äºåå‘ä¼ æ’­ä¸­çš„æ¯ä¸ª transformer å—ï¼š
    - AllGatherï¼šæ¯ä¸ª rank $16h^2/DP$ å­—èŠ‚
    - ReduceScatterï¼šæ¯ä¸ª rank $16h^2/DP$ å­—èŠ‚
- æ¯ä¸ªå—çš„æ€»é€šä¿¡ï¼š $3 \cdot 16h^2/DP$ å­—èŠ‚
- æ•´ä¸ªæ¨¡å‹çš„æ€»é€šä¿¡ï¼š $3 \cdot \text{num\_layers} \cdot 16h^2/DP$ å­—èŠ‚

AllGather çš„é€šä¿¡æ—¶é—´æ˜¯ï¼š

$$
t_{\text{comm}} = 16h^2 \cdot \frac{DP - 1}{DP \cdot \text{peak\_bw}}Â 
$$

ä¸€ä¸ªè§£ç å™¨å±‚çš„å‰å‘ä¼ æ’­çš„è®¡ç®—æ—¶é—´æ˜¯ï¼š

$$
t_{\text{compute}} = \frac{32 \cdot \text{seq\_len} \cdot \text{mbs} \cdot h^2}{\text{peak\_flops}}Â 
$$

ä¸ºäº†æœ‰æ•ˆåœ°åœ¨è®¡ç®—å’Œé€šä¿¡ä¹‹é—´è¿›è¡Œé‡å ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

$$
\frac{t_{\text{comm}}}{t_{\text{compute}}} = \frac{1}{2 \cdot \text{seq\_len} \cdot \text{mbs}} \cdot \frac{DP - 1}{DP} \cdot \frac{\text{peak\_flops}}{\text{peak\_bw}} \leq 1Â 
$$

å½“è¿™ä¸ªæ¯”ç‡å°äº 1 æ—¶ï¼Œä¸‹ä¸€å±‚çš„å‚æ•°é€šä¿¡å¯ä»¥éšè—åœ¨å½“å‰å±‚çš„è®¡ç®—ä¹‹åã€‚

### TP é€šä¿¡åˆ†æ

å¯¹äºå¼ é‡å¹¶è¡Œ (TP)ï¼Œåœ¨çº¿æ€§å±‚æœŸé—´æ¿€æ´»å€¼åœ¨ GPU ä¹‹é—´è¢«åˆ†ç‰‡ã€‚è®©æˆ‘ä»¬åˆ†æé€šä¿¡æ¨¡å¼ï¼š

- å¯¹äºå‰å‘ä¼ æ’­ä¸­çš„æ¯ä¸ªåˆ—çº¿æ€§å±‚ï¼š
    - AllGather æ¿€æ´»å€¼ï¼šæ¯ä¸ª rank  $seq \cdot mbs \cdot h/TP$ å­—èŠ‚
- å¯¹äºåå‘ä¼ æ’­ä¸­çš„æ¯ä¸ªåˆ—çº¿æ€§å±‚ï¼š
    - ReduceScatterï¼šæ¯ä¸ª rank $seq \cdot mbs \cdot h/TP$ å­—èŠ‚
- å¯¹äºè¡Œçº¿æ€§å±‚åä¹‹äº¦ç„¶ã€‚æ¯ä¸ª transformer å—æœ‰ 2 ä¸ªåˆ—çº¿æ€§å±‚å’Œ 2 ä¸ªè¡Œçº¿æ€§å±‚ã€‚
- æ¯ä¸ªå—çš„æ€»é€šä¿¡ï¼š $8 \cdot seq \cdot mbs \cdot h/TP$ å­—èŠ‚
- æ•´ä¸ªæ¨¡å‹çš„æ€»é€šä¿¡ï¼š $8 \cdot \text{num\_layers} \cdot seq \cdot mbs \cdot h/TP$ å­—èŠ‚

è®©æˆ‘ä»¬åˆ†ææˆ‘ä»¬æ˜¯å¦å¯ä»¥å°†ä¸€å±‚çš„æ”¶é›†å™¨é€šä¿¡ä¸ä¸‹ä¸€å±‚çº¿æ€§å±‚çš„è®¡ç®—é‡å ã€‚æ”¶é›†æ“ä½œçš„é€šä¿¡æ—¶é—´æ˜¯ï¼š

$$
t_{\text{comm}} = \frac{seq \cdot mbs \cdot h \cdot (TP - 1)}{TP \cdot \text{peak\_bw}}Â 
$$

è€Œä¸‹ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆå…·æœ‰å‚æ•° $h^2$ï¼‰çš„è®¡ç®—æ—¶é—´æ˜¯ï¼š

$$
t_{\text{compute}} = \frac{2 \cdot seq \cdot mbs \cdot h^2}{TP \cdot \text{peak\_flops}}Â 
$$

ä¸ºäº†æœ‰æ•ˆé‡å ï¼Œæˆ‘ä»¬å¸Œæœ›é€šä¿¡æ—¶é—´å°äºè®¡ç®—æ—¶é—´ï¼š

$$
\frac{t_{\text{comm}}}{t_{\text{compute}}} = \frac{TP - 1}{2 \cdot h} \cdot \frac{\text{peak\_flops}}{\text{peak\_bw}} \leq 1Â 
$$

è¿™ä¸ªæ¯”ç‡å‘Šè¯‰æˆ‘ä»¬æˆ‘ä»¬æ˜¯å¦å¯ä»¥æˆåŠŸåœ°å°†æ”¶é›†å™¨é€šä¿¡éšè—åœ¨ä¸‹ä¸€ä¸ªçº¿æ€§å±‚çš„è®¡ç®—ä¹‹åã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™ä¸ªæ¯”ç‡ä»…å–å†³äºéšè—å¤§å° $h$ å’Œå¼ é‡å¹¶è¡Œåº¦ TPï¼Œè€Œä¸æ˜¯åºåˆ—é•¿åº¦æˆ–æ‰¹é‡å¤§å°ã€‚

### PP é€šä¿¡åˆ†æ

å¯¹äºæµæ°´çº¿å¹¶è¡Œ (PP)ï¼Œæ¿€æ´»å€¼å’Œæ¢¯åº¦åœ¨æµæ°´çº¿é˜¶æ®µä¹‹é—´è¿›è¡Œé€šä¿¡ã€‚è®©æˆ‘ä»¬åˆ†æé€šä¿¡æ¨¡å¼ï¼š

- å¯¹äºå‰å‘ä¼ æ’­ä¸­çš„æ¯ä¸ªå¾®æ‰¹æ¬¡ï¼š
    - æ¥æ”¶å’Œå‘é€æ¿€æ´»å€¼ï¼š $2 \cdot seq \cdot mbs \cdot h$ å­—èŠ‚
- å¯¹äºåå‘ä¼ æ’­ä¸­çš„æ¯ä¸ªå¾®æ‰¹æ¬¡ï¼š
    - æ¥æ”¶å’Œå‘é€æ¢¯åº¦ï¼š $2 \cdot seq \cdot mbs \cdot h$ å­—èŠ‚
- æ¯ä¸ªMicro Batchçš„æ€»é€šä¿¡ï¼š $4 \cdot seq \cdot mbs \cdot h$ å­—èŠ‚
- å¯¹äºæ¢¯åº¦ç´¯ç§¯æ­¥éª¤ (gas)ï¼Œæ€»é€šä¿¡ï¼š $4 \cdot gas \cdot seq \cdot mbs \cdot h$ å­—èŠ‚

è®©æˆ‘ä»¬åˆ†ææˆ‘ä»¬æ˜¯å¦å¯ä»¥å°†æ¿€æ´»å€¼/æ¢¯åº¦çš„é€šä¿¡ä¸ä¸‹ä¸€ä¸ª transformer å—çš„è®¡ç®—é‡å ã€‚ä¸‹ä¸€ä¸ªæµæ°´çº¿é˜¶æ®µä¸­ transformer å—çš„è®¡ç®—æ—¶é—´æ˜¯ï¼š

$$
t_{\text{compute}} = \frac{32 \cdot seq \cdot mbs \cdot h^2 \cdot \text{num\_layers\_in\_next\_pp}}{\text{peak\_flops}}Â 
$$

è€Œ P2P ä¼ è¾“çš„é€šä¿¡æ—¶é—´æ˜¯ï¼š

$$
t_{\text{comm}} = \frac{seq \cdot mbs \cdot h}{\text{peak\_bw}}Â 
$$

ä¸ºäº†æœ‰æ•ˆé‡å ï¼Œæˆ‘ä»¬å¸Œæœ›ï¼š

$$
\frac{t_{\text{comm}}}{t_{\text{compute}}} = \frac{\text{peak\_flops}}{32 \cdot h \cdot \text{num\_layers\_in\_next\_pp} \cdot \text{peak\_bw}} \leq 1Â 
$$

ä¸ TP ç±»ä¼¼ï¼Œè¿™ä¸ªæ¯”ç‡ä¸åºåˆ—é•¿åº¦å’Œæ‰¹é‡å¤§å°æ— å…³ã€‚å®ƒå–å†³äºéšè—å¤§å° $h$ï¼Œä¸‹ä¸€ä¸ªæµæ°´çº¿é˜¶æ®µä¸­çš„å±‚æ•°ï¼Œä»¥åŠè®¡ç®—ä¸ç¡¬ä»¶ P2P å¸¦å®½èƒ½åŠ›çš„æ¯”ç‡ã€‚

> ç¼–è€…æ³¨ï¼šå…¨ä¹¦ç»“æŸï¼Œæ„Ÿè°¢é˜…è¯»ã€‚
> 

## Reference

[1] [https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp](https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp)

[2] [https://pytorch.org/docs/stable/distributed.html#which-backend-to-use](https://pytorch.org/docs/stable/distributed.html#which-backend-to-use)